# Kafka Connect SQS Source Connector - Decompression with Avro
#
# This example demonstrates decompressing messages and then converting to
# Avro format with Schema Registry integration.
#
# Use case: Compressed messages that need schema validation and Avro
# serialization for efficient storage and processing in Kafka.

# Connector Configuration
name=sqs-decompression-avro-connector
connector.class=io.connect.sqs.SqsSourceConnector
tasks.max=1

# AWS Configuration
aws.region=us-east-1
# Leave empty to use IAM role / instance profile
aws.access.key.id=
aws.secret.access.key=

# SQS Configuration
sqs.queue.url=https://sqs.us-east-1.amazonaws.com/123456789/compressed-avro-messages
sqs.max.messages=10
sqs.wait.time.seconds=20
sqs.visibility.timeout.seconds=30
sqs.delete.messages=true

# Kafka Configuration
kafka.topic=avro-messages-decompressed
kafka.topic.partition=0

# SCRAM Authentication (Mandatory)
sasl.mechanism=SCRAM-SHA-512
security.protocol=SASL_SSL
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="kafka-connect-user" \
  password="${env:KAFKA_PASSWORD}";

# Message Decompression Configuration
# Step 1: Decompress the message
message.converter.class=io.connect.sqs.converter.DecompressingMessageConverter

# Step 2: Convert decompressed JSON to Avro
message.decompression.delegate.converter.class=io.connect.sqs.converter.AvroMessageConverter

# Decompress nested field (e.g., EventBridge events)
message.decompression.field.path=detail.data

# Auto-detect compression format
message.decompression.format=AUTO

# Enable Base64 decoding
message.decompression.base64.decode=true

# Schema Registry Configuration
schema.registry.url=http://schema-registry:8081

# Auto-register schemas inferred from JSON structure
schema.auto.register=true

# Use topic-based naming strategy for schemas
schema.subject.name.strategy=io.confluent.kafka.serializers.subject.TopicNameStrategy

# Optional: Specify schema ID if already registered
# value.schema.id=100

# Optional: Schema Registry authentication
# schema.registry.basic.auth.user.info=username:password
# schema.registry.basic.auth.credentials.source=USER_INFO

# Error Handling
dlq.topic=avro-messages-dlq
max.retries=3
retry.backoff.ms=1000

# Processing Flow:
# 1. Receive compressed message from SQS
# 2. DecompressingMessageConverter decompresses detail.data
# 3. AvroMessageConverter infers Avro schema from decompressed JSON
# 4. Schema is registered with Schema Registry (if auto.register=true)
# 5. Message is serialized to Avro format
# 6. Avro-serialized message is sent to Kafka topic
#
# Example EventBridge message:
# {
#   "version": "0",
#   "detail-type": "Order.Created",
#   "detail": {
#     "data": "H4sIAAAAAAAA/6tWKkktLlGyUlAqS8wpTtVRKi1OLYrPTSwpSs2zUgKpBQBZvhNoIwAAAA=="
#   }
# }
#
# After decompression, detail.data contains:
# {"orderId":12345,"customerId":67890,"amount":99.99,"currency":"USD"}
#
# This is then converted to Avro and sent to Kafka with schema validation.
