# Kafka Connect SQS Source Connector - EventBridge Complete Pipeline
#
# This example demonstrates the complete message processing pipeline:
# 1. Retrieve large messages from S3 (Claim Check Pattern)
# 2. Decompress gzip/Base64-encoded data
# 3. Extract nested business data fields (Field Extraction)
#
# Use case: EventBridge events with compressed data stored in S3, where you only
# want the business data (detail.data) in Kafka without the EventBridge envelope.

# Connector Configuration
name=sqs-eventbridge-complete-connector
connector.class=io.connect.sqs.SqsSourceConnector
tasks.max=1

# AWS Configuration
aws.region=eu-west-1
# Leave empty to use IAM role / instance profile
aws.access.key.id=
aws.secret.access.key=

# SQS Configuration
sqs.queue.url=https://sqs.eu-west-1.amazonaws.com/123456789/eventbridge-events
sqs.max.messages=10
sqs.wait.time.seconds=10
sqs.visibility.timeout.seconds=30
sqs.delete.messages=true

# Kafka Configuration
kafka.topic=flight-offers-data
# Optional: specify partition
# kafka.topic.partition=0

# SCRAM Authentication (Mandatory)
sasl.mechanism=SCRAM-SHA-512
security.protocol=SASL_SSL
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="kafka-connect-user" \
  password="${env:KAFKA_PASSWORD}";

# Message Converter Configuration
# Use DecompressingClaimCheckMessageConverter for the complete pipeline
message.converter.class=io.connect.sqs.converter.DecompressingClaimCheckMessageConverter

# Delegate to DefaultMessageConverter after all processing
message.decompression.delegate.converter.class=io.connect.sqs.converter.DefaultMessageConverter

# Decompression Configuration
# The detail.data field contains gzip-compressed, Base64-encoded JSON
message.decompression.field.path=detail.data
message.decompression.format=AUTO
message.decompression.base64.decode=true

# Claim Check Configuration
# If detail.data contains an S3 URI, retrieve the content from S3
message.claimcheck.field.path=detail.data
message.claimcheck.retrieve.if.uri=true

# Field Extraction Configuration
# Extract only the detail.data field, removing the EventBridge envelope
message.output.field.extract=detail.data
message.output.field.extract.failOnMissing=false

# Error Handling
dlq.topic=flight-offers-dlq
max.retries=3
retry.backoff.ms=1000

# Processing Pipeline Explanation:
#
# Step 1: Message received from SQS
# {
#   "version": "0",
#   "id": "event-123",
#   "detail-type": "FlightOffersUpdate",
#   "source": "OffersService",
#   "detail": {
#     "data": "s3://my-bucket/offers/compressed-data.json.gz"  OR
#     "data": "H4sIAAAAAAAA/6tWKkktLlGyUlAqS8wpTtVRKi1OLYrPTSwpSs2zUgKpBQBZvhNoIwAAAA=="
#   }
# }
#
# Step 2: Claim Check - If detail.data is S3 URI, retrieve from S3
# {
#   "version": "0",
#   "id": "event-123",
#   "detail-type": "FlightOffersUpdate",
#   "source": "OffersService",
#   "detail": {
#     "data": "H4sIAAAAAAAA/6tWKkktLlGyUlAqS8wpTtVRKi1OLYrPTSwpSs2zUgKpBQBZvhNoIwAAAA=="
#   }
# }
#
# Step 3: Decompression - Decompress the gzip/Base64 data in detail.data
# {
#   "version": "0",
#   "id": "event-123",
#   "detail-type": "FlightOffersUpdate",
#   "source": "OffersService",
#   "detail": {
#     "data": {
#       "offers": [
#         {"id": "456", "price": 250.00, "airline": "IB"}
#       ]
#     }
#   }
# }
#
# Step 4: Field Extraction - Extract detail.data, remove EventBridge envelope
# Final output sent to Kafka:
# {
#   "offers": [
#     {"id": "456", "price": 250.00, "airline": "IB"}
#   ]
# }
#
# This makes your EventBridge topic (raw.1) output match your direct data topic (raw.0)
