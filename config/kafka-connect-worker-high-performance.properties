# Example Kafka Connect Worker Configuration for High Performance
#
# This file shows recommended settings for high-throughput scenarios with large messages.
# Copy relevant settings to your actual worker configuration file:
# - Distributed mode: connect-distributed.properties
# - Standalone mode: connect-standalone.properties
#
# Use case: EventBridge messages with compression, S3 claim check, and field extraction
# Target: 1500+ msg/sec with messages up to 10MB

# ============================================================================
# Basic Kafka Connect Configuration
# ============================================================================

# Kafka brokers
bootstrap.servers=b-1.your-msk-cluster.kafka.region.amazonaws.com:9096

# Connect cluster identification
group.id=kafka-connect-cluster

# Internal topics for Connect cluster coordination
config.storage.topic=_connect_configs
offset.storage.topic=_connect_offsets
status.storage.topic=_connect_status

# Replication factors (adjust based on broker count)
config.storage.replication.factor=3
offset.storage.replication.factor=3
status.storage.replication.factor=3

# Plugin path (where connectors are installed)
plugin.path=/opt/kafka/plugins/,/usr/share/java/,/usr/share/confluent-hub-components/

# REST API configuration
rest.advertised.host.name=localhost
rest.port=8083

# ============================================================================
# Converters (how data is serialized)
# ============================================================================

# Key converter (usually String)
key.converter=org.apache.kafka.connect.storage.StringConverter

# Value converter (adjust based on your needs)
value.converter=org.apache.kafka.connect.storage.StringConverter
# OR for Avro:
# value.converter=io.confluent.connect.avro.AvroConverter
# value.converter.schema.registry.url=http://schema-registry:8081

# Internal converters (used by Connect framework)
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter

# ============================================================================
# Security Configuration (SCRAM-SHA-512)
# ============================================================================

security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-512
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="admin" \
  password="your-password";

# ============================================================================
# Consumer Configuration (for offset storage and source connectors)
# ============================================================================

consumer.security.protocol=SASL_SSL
consumer.sasl.mechanism=SCRAM-SHA-512
consumer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="admin" \
  password="your-password";

# Consumer performance (optional)
consumer.max.poll.records=500
consumer.max.poll.interval.ms=300000

# ============================================================================
# Producer Configuration - CRITICAL FOR LARGE MESSAGES
# ============================================================================

producer.security.protocol=SASL_SSL
producer.sasl.mechanism=SCRAM-SHA-512
producer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
  username="admin" \
  password="your-password";

# Large Message Support
# IMPORTANT: Increase these to handle large messages from decompression/claim check
producer.max.request.size=10485760
# 10MB - Handles messages up to 10MB (default is 1MB)
# Adjust higher if your messages are larger (e.g., 20971520 for 20MB)

producer.buffer.memory=67108864
# 64MB - Total memory buffer for producer (default is 32MB)
# Increase for high-throughput scenarios

# Performance Optimization
producer.compression.type=snappy
# Options: none, gzip, snappy, lz4, zstd
# snappy: Fast compression with reasonable ratio (recommended)
# gzip: Better compression but slower
# lz4: Fastest compression
# none: No compression (higher network usage)

producer.batch.size=262144
# 256KB - Batch size for better throughput (default is 16KB)
# Larger batches = better throughput, slightly higher latency

producer.linger.ms=10
# 10ms - Small delay to allow batching (default is 0)
# Improves throughput by batching multiple records
# Increase to 20-50ms for even better throughput (with higher latency)

producer.acks=1
# Acknowledgment level:
# 0 = No acknowledgment (fastest, risk of data loss)
# 1 = Leader acknowledgment only (balanced - RECOMMENDED)
# all = All replicas acknowledgment (slowest, most durable)

# Additional producer tuning (optional)
producer.send.buffer.bytes=131072
# 128KB - Socket send buffer
producer.receive.buffer.bytes=32768
# 32KB - Socket receive buffer

producer.request.timeout.ms=30000
# 30s - Timeout for requests
producer.retry.backoff.ms=100
# 100ms - Backoff between retries

# ============================================================================
# Connect Worker Tuning (optional)
# ============================================================================

# Task concurrency
# offset.flush.interval.ms=60000
# offset.flush.timeout.ms=5000

# Heartbeat and session timeouts
# heartbeat.interval.ms=3000
# session.timeout.ms=30000

# ============================================================================
# High-Throughput Configuration Summary
# ============================================================================
#
# With these settings, the worker supports:
# - Messages up to 10MB (increase producer.max.request.size for larger)
# - Throughput of 1500+ msg/sec with proper connector configuration
# - Efficient batching and compression
# - Balanced durability and performance
#
# For connector-specific high throughput:
# - Use multi-queue configuration (tasks.max=15+)
# - Optimize polling (poll.interval.ms=50-100)
# - Maximize batch size (sqs.max.messages=10)
#
# Example connector config for 1500 msg/sec:
# {
#   "name": "sqs-high-throughput",
#   "connector.class": "io.connect.sqs.SqsSourceConnector",
#   "tasks.max": "15",
#   "sqs.queue.urls": "queue-0,queue-1,...queue-14",
#   "sqs.max.messages": "10",
#   "poll.interval.ms": "50",
#   "message.converter.class": "io.connect.sqs.converter.DecompressingClaimCheckMessageConverter",
#   "message.output.field.extract": "detail.data"
# }
#
# ============================================================================
# Troubleshooting
# ============================================================================
#
# RecordTooLargeException:
# - Increase producer.max.request.size
# - Check message size after decompression
# - May also need to increase Kafka broker message.max.bytes
#
# Low Throughput:
# - Increase tasks.max (one task per queue)
# - Decrease poll.interval.ms
# - Increase producer.batch.size and producer.linger.ms
#
# High Latency:
# - Decrease producer.linger.ms
# - Decrease poll.interval.ms
# - Use fewer queues with higher message volume per queue
#
# Out of Memory:
# - Decrease producer.buffer.memory
# - Decrease producer.batch.size
# - Reduce number of tasks
#
# ============================================================================
